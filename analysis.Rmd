---
title: "RandomTree for `classe` prediction"
output:
  html_notebook: default
  html_document: default
---

We build a RandomTree model for predicting the `"classe"` feature on the **Weight Lifting Dataset** from [Groupware@LES](http://groupware.les.inf.puc-rio.br/har).

## Library & Data

We load the `caret` library and get the data:

```{r}
library(caret)
urlTrain<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlTest<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(urlTrain,"train.csv",method="curl",quiet=TRUE)
download.file(urlTest,"test.csv",method="curl",quiet=TRUE)
trainSave<-read.csv("train.csv",na.strings=c("NA",""))
testSave<-read.csv("test.csv",na.strings=c("NA",""))
train<-trainSave
test<-testSave
```

## Cleaning the Data

We find all completely empty columns from the dataset and remove them.
Then we scan `train` and `test` to find any remaining `NA` entries:

```{r}
naColumns<-which(as.vector(colSums(is.na(test)))==nrow(test))
train<-train[,-naColumns]
test<-test[,-naColumns]
which(as.vector(colSums(is.na(test)))>0 | as.vector(colSums(is.na(train)))>0)
```

Since there aren't any remaining `NA` entries, we can continue.

After looking at the output of `str(train)`, we decide to remove the following columns from the dataset

`X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp,new_window,num_window`

`X` is just an index, `user_name` identifies the participant, and the `*time*` and `*window*` features should not influence the activity class.

```{r}
removeCol<-function(data){
  subset( data, select = -c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,
             cvtd_timestamp,new_window,num_window))
}

train<-removeCol(train)
test<-removeCol(test)
```

Anything else we should remove?

```{r}
nearZeroVar(train)
```

All the remaining variables are significant.

Before building predictive models, we set aside a `testingSet` (since the provided test data is missing the `"classe"` column).

```{r}
trainIndex <- createDataPartition(train$classe, p = .9,list = FALSE)
trainingSet <- train[trainIndex,]
testingSet<- train[-trainIndex,]
```

We are ready to build predictive models!

## Building a Random Forest

We first build a model via a generic random forest algorithm.

We use the `train` function from the `caret` library to build a random forest (`model="rf"`) on the `trainingSet` dataset. We set a cross-validation via the `method="cv"` in the `control` parameter. (The number of fold in the cross-validation is the default, i.e., `10`).

```{r}
control <- trainControl(method="cv")
classeIndex<-which(names(train)=="classe")
fitRF<-train(trainingSet[,-classeIndex],trainingSet[,classeIndex],model="rf",trControl=control)
```

## Cross-validating the model / out-of-sample error

We are going to consider two kinds of error measurements:

1. The accuracy from the 10-fold cross-validation (which was performed automatically by the `train` function with the `"cv"` control-parameter).

2. The accuracy from the manual data split that we did (we split `train` into a `trainingSet` and a `testingSet`).
 
### 10-fold cross validation

Let us first look at the accuracy automatically computed via the cross-validation (=the `"cv"` parameter)

```{r}
print(fitRF)
```

The highest 10-fold cross-validation accuracy of `0.994734` was obtained with the tuning parameter `mtry=2` (=number of variables randomly sampled as candidates at each split).

### Data-Split

To have a truly "out-of-sample" measurement of the error, let us compare the cross-validation accuracy to that of a simple data-split. I.e., we build the confusion matrix for the prediction of our model vs. the `testingSet` (that we haven't used in the `train` function)

```{r}
predictions<-predict(fitRF,testingSet)
confusionMatrix(predictions, testingSet[,"classe"])
```


Hence, the out-of-sample accuracy computed via data split is `0.9939` (very close to the `0.994734` obtained via cross-validation).

## Prediction on the `test` data

```{r}
predict(fitRF,test)
```

(That's a 100% accuracy once submitted to the Quizz)
